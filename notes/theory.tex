\documentclass[paper=a4,11pt,headsepline]{scrartcl}

\usepackage[margin=2cm]{geometry}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{minted}
\usepackage{booktabs}
\usepackage{url}
\usepackage[colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black,hyperfootnotes=true]{hyperref}
\usepackage{cancel}
\usepackage{xspace}

\newcommand{\ve}[1]{\ensuremath{\bm{\mathit{#1}}}}
\newcommand{\ma}[1]{\ensuremath{\bm{\mathbf{#1}}}}
\newcommand{\dd}{\text{d}}
\newcommand{\ra}{\ensuremath{\rightarrow}}
\newcommand{\la}{\ensuremath{\leftarrow}}
\newcommand{\pd}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\td}[2]{\dfrac{\dd #1}{\dd #2}}
\newcommand{\pdi}[2]{\partial #1/\partial #2}
\newcommand{\tdi}[2]{\dd #1/\dd #2}
\newcommand{\red}[1]{{\color{red}{#1}}}
\newcommand{\green}[1]{{\color{green}{#1}}}
\newcommand{\blue}[1]{{\color{blue}{#1}}}
\newcommand{\ts}[1]{\textsuperscript{#1}}
\newcommand{\soft}[1]{\textsl{#1}\xspace}
\newcommand{\numpy}{\soft{numpy}}
\newcommand{\pytorch}{\soft{pytorch}}
\newcommand{\jax}{\soft{jax}}
\newcommand{\autograd}{\soft{autograd}}
\newcommand{\scipy}{\soft{scipy}}
\newcommand{\numdifftools}{\soft{numdifftools}}
\newcommand{\tf}{\soft{tensorflow}}
\newcommand{\co}[1]{\texttt{#1}}

\begin{document}

\section{Methods to evaluate derivatives}
%
Taken (mostly) from \cite{baydin_2018}.

\begin{itemize}
    \item work out derivs by hand (or really Mathematica, Maple, Maxima, ...)
        and code them
    \item call symbolic engines (e.g. \soft{sympy}) in your code
    \begin{itemize}
        \item special sub-type: manually construct computation graph by
            framework mini-language (TensorFlow, Theano) (e.g. \co{tf.while},
            \co{tf.cond}), the framework then does symbolic derivs and thus
            creates a new computation graph for derivs which is used in SGD
    \end{itemize}
\item numerical derivs by finite differences (FD), use at least central diffs method,
    good for checking AD gradients, scales as $\mathcal
    O(n)$ for $\nabla f(\ve x), \ve x\in\mathbb R^n$, slow when $n\sim 10^6$,
    Python packages: \numdifftools, \scipy
\item AD
    \begin{itemize}
        \item directly provides numerical values for derivatives at some point
            $\ve x$ with machine precision, \emph{not} symbolic expressions of
            derivatives
        \item uses operator overloading (or other methods) to carry out derivs
        \item can be applied to almost any code using arbitrary control flows
        \item in reverse mode uses code tracing ("taping") by one forward eval
            to construct computation graph
        \item no "code swell": complex expressions that grow which each
            application of $\partial/\partial x_i$ in symbolic engines and that
            need an additional simplification step (Theano does this)
        \item limitations
        \begin{itemize}
            \item in-place ops such as \co{A[i,j] *= 3} instead of
                \co{B=A*3} are hard to support and can slow things down b/c
                large parts of the comp graph must be rewritten instead of just
                adding a new variable; in \autograd they aren't even
                supported at all; \jax and \pytorch have a jit,
                which should optimize out-of-place modifications and copies
                away
        \end{itemize}
    \end{itemize}
\end{itemize}
%
\section{Setting the stage}
%
First consider a scalar function of scalar inputs $f:\mathbb R\ra\mathbb R$
\begin{equation*}
    f(x) = c(b(a(x)))
\end{equation*}
which gives the straight graph $x\ra a\ra b\ra c = f$. Forward = $x \ra c$ (inputs
to outputs). Reverse = $x \la c$ (outputs to inputs).
The derivative using the chain rule
\begin{align*}
    \td{f}{x}
        &= \td{c}{b}\,\td{b}{a}\,\td{a}{x}\\
        &= \td{c}{b}\,\left(\td{b}{a}\,\td{a}{x}\right)\quad\text{forward}\\
        &= \left(\td{c}{b}\,\td{b}{a}\right)\,\td{a}{x}\quad\text{reverse}\\
\end{align*}

\section{Forward and reverse AD (in \jax)}
%
Now we consider a multivariate vector function $\ve f: \mathbb R^n \ra \mathbb R^m$.
The Jacobian is
\begin{equation*}
    \ve f(\ve x): \mathbb R^n \ra \mathbb R^m\quad \ma J =
        \begin{pmatrix}
            \pd{f_1}{x_1} & \cdots & \pd{f_1}{x_n}  \\
            \vdots        & \ddots & \vdots         \\
            \pd{f_m}{x_1} & \cdots & \pd{f_m}{x_n}
        \end{pmatrix}
    \in\mathbb R^{m\times n}
\end{equation*}
One edge case is
\begin{equation*}
    \ve f(x): \mathbb R^1 \ra \mathbb R^m\quad \ma J =
        \begin{pmatrix}
            \pd{f_1}{x_1}  \\
            \vdots         \\
            \pd{f_m}{x_1}
        \end{pmatrix}
    \in\mathbb R^{m\times 1}
\end{equation*}
where we need only the first column $\ma J[:,1]$. In the other edge case
\begin{equation*}
f(\ve x): \mathbb R^n \ra \mathbb R^1\quad \ma J =
    \begin{pmatrix}
        \pd{f_1}{x_1} & \cdots & \pd{f_1}{x_n}
    \end{pmatrix}
    \in\mathbb R^{1\times n}
    \equiv \nabla f\equiv \pd{f}{\ve x}
\end{equation*}
we need the first row $\ma J[1,:]$ only.

\subsection{Vectorized \numpy functions}

Vectorized \numpy functions $f$ are a bit of a special case, since depending on
input, they are either $f(x):\mathbb R\ra\mathbb R$ or $\ve f(\ve x):\mathbb
R^n\ra\mathbb R^n$, but never $f(\ve x): \mathbb R^n \ra \mathbb R$ (i.e. the
gradient $\nabla f(\ve x)$ is not defined for them). Also $f_i\equiv f$. Since
each $f_i$ is a function of only the $x_i$ it is applied to, the Jacobian is
always diagonal, which is a very neat property that we'll use later.

\subsection{Forward}

Again we have
\begin{equation*}
    \ve f(\ve x) = \ve c(\ve b(\ve a(\ve x)))
\end{equation*}
and can use the chain rule
\begin{equation*}
    \ma J=\pd{\ve f}{\ve x} = \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}
\end{equation*}
which would give a series of matrix multiplications of individual Jacobians $\pdi{\ve
a}{\ve x}$, $\pdi{\ve b}{\ve a}$, $\pdi{\ve c}{\ve b}$.
Note that we can right-multiply $\ma J$ with the identity
matrix $\pdi{\ve x}{\ve x}$
\begin{equation*}
    \pd{\ve f}{\ve x}\,\pd{\ve x}{\ve x} =
    \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\,\pd{\ve x}{\ve x}
\end{equation*}
which doesn't change anything. We can also think of this as extracting the
$j$-th column $\ma J[:,j]$ by multiplying from the right with one column of the
identity $\partial\ve x/\partial x_j = \ve e_j = [0,0,\cdots,1,\cdots, 0]$.
This will thus select the $x_j$ w.r.t. to which we want to calculate the
derivative. Analytically this means for an example $2\times 2$ system and choosing
$x_j = x_1$
\begin{equation*}
    \pd{\ve f}{\ve x}\,\pd{\ve x}{x_1}=
    \begin{pmatrix}
        \pd{f_1}{x_1} & \pd{f_1}{x_2} \\
        \pd{f_2}{x_1} & \pd{f_2}{x_2} \\
    \end{pmatrix}
    \begin{pmatrix}
        \pd{x_1}{x_1} \\
        \pd{x_2}{x_1} \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        \pd{f_1}{x_1}\,\pd{x_1}{x_1} + \cancelto{0}{\pd{f_1}{x_2}\,\pd{x_2}{x_1}} \\
        \pd{f_2}{x_1}\,\pd{x_1}{x_1} + \cancelto{0}{\pd{f_2}{x_2}\,\pd{x_2}{x_1}} \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        \pd{f_1}{x_1} \\
        \pd{f_2}{x_1} \\
    \end{pmatrix}
\end{equation*}
if we note that $\pdi{x_2}{x_1}=0$ and that formally by the chain rule
\begin{gather*}
    \pd{f_1}{x_1}\,\pd{x_1}{x_1} = \pd{f_1}{x_1} \\
    \pd{f_2}{x_1}\,\pd{x_1}{x_1} = \pd{f_2}{x_1} \\
\end{gather*}
Now, of course we don't want to build up Jacobians. Also we haven't done any AD
yet, have we? The trick is to evaluate the expression by using ($i$)
pre-defined derivatives of \emph{primitive} functions (here $\ve a(\cdot)$,
$\ve b(\cdot)$, $\ve c(\cdot)$)%
\footnote{Any basic (\numpy) function like \co{sin}, \co{sum}
or \co{sqrt} that we can't or won't represent in terms of even more basic
functions.}
and ($ii$) by repeated application of a
\blue{\emph{Jacobian vector product (JVP)}}. So, we initialize with $\ve e_j$
and apply JVPs as we go.
\begin{align*}
    \pd{\ve f}{\ve x}\,\pd{\ve x}{x_j}
        &= \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\blue{\pd{\ve a}{\ve x}\,\ve e_j} \\
        &= \pd{\ve c}{\ve b}\,\blue{\pd{\ve b}{\ve a}\,\ve v_j} \\
        &= \blue{\pd{\ve c}{\ve b}\,\ve v_j'} \\
        &= \ma J[:,j]
\end{align*}
Now what is a JVP? We see that in order to evaluate the chain rule, each
primitive function
needs to know ($i$) how it would evaluate its own derivative (Jacobian, e.g.
$\pdi{\ve b}{\ve a}$) w.r.t. its inputs and ($ii$) how to multiply that with a
given vector. By doing ($ii$), we can always initialize the process by $\ve
e_j$ and then multiply forward. Cool! Now how do we do $(i)$? The answer is
that we don't (almost). There is a function \co{jax.jvp} that, for a given
primitive function $\ve a(\ve x)$, maps a so-called
\emph{primal}-\emph{tangent} pair $(\ve x, \ve v)$
\begin{equation*}
    \co{jvp}: (\ve x, \ve v) \ra \left(\ve a(\ve x), \ma J(\ve x)\,\ve v\right)
\end{equation*}
to the function at $\ve x$ and its JVP with $\ve v$. This is very roughly how
\jax works inside. Each primitive function has an associated
JVP function defined by using \co{jax.defjvp} which gets called when evaluating
the chain rule. \textbf{The most important trick is that the intermediate
Jacobians are never explicitly build}. The JVP function only has to return the
\emph{result} of $\ma J(\ve x)\,\ve v$. Especially, for \numpy's vectorized
functions, all Jacobians are diagonal, which makes evaluating the result of $\ma J(\ve x)\,\ve v$
simple and efficient. In most cases, we get away with element-wise operations
on \ve v.

Side note: in the AD literature, applying JVPs is also called
"push-forward" of the vector (here $\ve e_j$) from inputs to outputs ($\ve x\ra\ve c$).

When we repeat the process and apply all possible $\ve e_j$ (i.e. the whole
identity matrix), we can build up the full $\ma J$ if we need to, one column at
a time. For the edge case $\ve f(x): \mathbb R^1 \ra \mathbb R^m$ ($n=1$, first column)
we are finished in one forward pass. In general forward mode is efficient for
"tall" Jacobians where $m\gg n$ and inefficient for "wide" ones where $m\ll n$.
The other edge case $f(\ve x): \mathbb R^n \ra \mathbb R^1$ ($m=1$, first row a.k.a.
$\nabla f(\ve x)$) is inefficient in forward mode since we need $n$ passes to
calculate each $\pdi{f}{x_j}$. This has thus similar complexity as finite
differences, where we also repeat $n$ times something like $(f(\ve x +\ve e_j\,h) - f(\ve
x-\ve e_j h))/2\,h$.

\subsection{Reverse}

First, we do a forward pass to trace the execution done in $\ve f$ to build up
the graph $\ve x\ra \ve a\ra \ve b\ra \ve c = \ve f$. Then we can extract
$i$-th row $\ma J[i,:]$ by multiplying from the left with $\pdi{\ve f}{f_i} =
\ve e_i$. Analytically, we have
\begin{equation*}
    \begin{split}
    \left(\pd{\ve f}{f_1}\right)^\top\,\pd{\ve f}{\ve x}
    =
    \begin{pmatrix}
        \pd{f_1}{f_1} & \pd{f_2}{f_1} \\
    \end{pmatrix}
    \begin{pmatrix}
        \pd{f_1}{x_1} & \pd{f_1}{x_2} \\
        \pd{f_2}{x_1} & \pd{f_2}{x_2} \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        \pd{f_1}{f_1}\,\pd{f_1}{x_1} + \cancelto{0}{\pd{f_2}{f_1}\,\pd{f_2}{x_1}} &
        \pd{f_1}{f_1}\,\pd{f_1}{x_2} + \cancelto{0}{\pd{f_2}{f_1}\,\pd{f_2}{x_2}}
    \end{pmatrix}
    =\\
    \begin{pmatrix}
        \pd{f_1}{x_1} & \pd{f_1}{x_2}
    \end{pmatrix}
    \end{split}
\end{equation*}
with $\pdi{f_2}{f_1}=0$.
In the code, we initialize with $\ve e_i$ and this time apply
\blue{\emph{vector Jacobian products (VJPs)}} as we go.
\begin{align*}
    \left(\pd{\ve f}{f_i}\right)^\top\,\pd{\ve f}{\ve x}
        &= \blue{\ve e_i^\top\,\pd{\ve c}{\ve b}}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\\
        &= \blue{\ve v_i^\top\,\pd{\ve b}{\ve a}}\,\pd{\ve a}{\ve x}\\
        &= \blue{\ve v_i'^\top\,\pd{\ve a}{\ve x}}\\
        &= \ma J[i,:]
\end{align*}
Again, we can apply all possible $\ve e_i$ to build up the full $\ma J$, this
time one row at a time. For the edge case $f(\ve x): \mathbb R^n \ra \mathbb
R^1$ ($m=1$, first row) we are done in one backward pass.

Side note: in the AD literature, applying VJPs is also called "pull-back" of
the vector (here $\ve e_i$) from outputs to inputs ($\ve x\la\ve c$).

Note that
\begin{equation*}
    \ve e_i^\top\,\ma J = \ma J^\top\,\ve e_i
\end{equation*}
i.e. use JVPs on $\ma J^\top$. This is how \jax does it, as far as I
can tell. In contrast to \autograd, in \jax they never define
VJPs but instead only JVPs and use the transpose trick to evaluate the reverse
mode. TODO: find out how \pytorch does it. For now we operate under the
assumption that they do VJPs as well as in \autograd.

\subsection{Reverse in ML a.k.a. backprop}

Typical in ML: a multivariate function $f(\ve x): \mathbb R^n\ra \mathbb R$
\begin{equation*}
    f(\ve x) = c(\ve b(\ve a(\ve x)))
\end{equation*}
for instance
\begin{minted}{python}
    f = lambda x: c(b(a(x)))
    f = lambda x: sum(power(sin(x),2))
\end{minted}
i.e. the composition of a number of vector-valued functions $\ve a(\cdot), \ve
b(\cdot)$ and a final reduction $c(\cdot): \mathbb R^n\ra \mathbb R$ and where
$n$ is "large" (as in $10^5$). For example, $f$ is the loss function in NN
training and $\ve x$ are the network's parameters. This is exactly the second
edge case from above. When using reverse mode, we get the gradient $\pdi{f}{\ve
x} = \ma J[1,:]$ in a single reverse pass (i.e. backprop).

\subsection{jax, autograd}

\begin{minted}{python}
    >>> x = np.random.rand(3)
    >>> grad(f)(x)
    DeviceArray([0.39920127, 0.8924804 , 0.05259493], dtype=float32)
\end{minted}

\jax returns \co{DeviceArray} objects because it used \tf's XLA compiler.
\autograd returns plain \numpy arrays.

\subsection{\pytorch}

\pytorch is not functional, it thinks in terms of \co{torch.Tensor} objects, so
when using our little example from above, we use the "loss tensor" \co{c} of
shape \co{(1,)} instead of the loss function \co{f}. With knowledge of reverse
mode, we can make sense of \pytorch's API design. Observe how each tensor has a
\verb|grad_fn| attribute which corresponds to the \co{jvp} (forward) or
\co{vjp} (backward) components in \jax.
%
\begin{minted}{python}
    # gradient of scalar result c w.r.t. x, evaluated at x
    # step by step, see grad_fn
    >>> x = torch.rand(3, requires_grad=True)

    ##c = x.sin().pow(2.0).sum()
    >>> a = torch.sin(x)
    tensor([0.7826, 0.2057, 0.5249], grad_fn=<SinBackward>)
    >>> b = torch.pow(a, 2.0)
    tensor([0.6125, 0.0423, 0.2755], grad_fn=<PowBackward0>)
    >>> c = torch.sum(b)
    tensor(0.9303, grad_fn=<SumBackward0>)
    >>> c.backward()
    >>> x.grad
    tensor([0.9743, 0.4026, 0.8935])
\end{minted}
%
In \pytorch the tracing of forward ops is implemented such that a tensor, here
the input \co{x}, records all operations applied to it. Then, we call the
\co{backward()} method of the final output tensor, here \co{c}. This
"pulls back" the derivatives (starting with $\pdi{f}{f}=1$) from the output
\co{c} to the input and the derivatives are stored in $\pdi{c}{\ve x} =
\co{x.grad}$.\footnote{This is super weird, pull-back or not.
\co{c.grad} would be much more intuitive.}

We can also perform VJPs by calling \co{backward()} of an intermediate
vector-valued variable and giving it a vector argument ($\ve
e$ or $\ve v$ above).
%
\begin{minted}{python}
    # VJP: extract one row of J
    >>> x = torch.rand(3, requires_grad=True)
    >>> v = torch.tensor([1.0,0,0])
    >>> b = x.sin().pow(2.0)
    >>> b.backward(v)
    >>> x.grad
    tensor([0.9619, 0.0000, 0.0000])
\end{minted}
And in particular the default for "seeding" the backward pass starting at \co{c}
is indeed \co{1.0} as in $\pdi{f}{f}=1$.
\begin{minted}{python}
    >>> c.backward(torch.tensor(1.0))
\end{minted}

\section{Comparison of functional diff implementations}

\begin{table}[h]
    \begin{tabular}{lllll}
        \toprule
        what                  & \scipy\ts{a}         & \numdifftools& \jax        & \autograd      \\
        \midrule
        $\dd f(x)/\dd x$      & \verb|M.gradient|            & \verb|Derivative|&   \verb|grad|            & \verb|grad|            \\
        $\nabla f(\ve x)$     & \verb|O.approx_fprime|       & \verb|Gradient|  &                          &                        \\
        \ts{b}$\nabla f(\ve x) = \text{map}(\dd f(x)/\dd x, \ve x)$ &&          &   \verb|vmap(grad(.))|   & \verb|elementwise_grad|\\
        $\pdi{\ve f}{\ve x}$  & \verb|N.approx_derivative|   & \verb|Jacobian|  &   \verb|jacobian|        & \verb|jacobian|        \\
        \bottomrule
    \end{tabular}
    \caption{\scipy, \numdifftools: FD; \jax, \autograd: AD\\
             \ts{a} \co{M=scipy.misc}, \co{O=scipy.optimize}, \co{N=optimize.\char`_numdiff}\\
             \ts{b} for vectorized \numpy functions where the Jacobian is diagonal:
             \co{diag(jacobian(sin)(x)) == cos(x)}}
\end{table}

\section{JVPs (forward) and VJPs (reverse) in \autograd an \jax}

\subsection{\autograd}

\begin{itemize}
    \item They define a VJP for "each" numpy function in
        \url{https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_vjps.py}
        using \co{defvjp}, also some JVPs in
        \url{https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_jvps.py}
        but \autograd is mostly only reverse mode
    \item numpy API definition
        \url{https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_wrapper.py}
\end{itemize}

\subsection{\jax}

\begin{itemize}
    \item numpy primitives defined (re-implemented using the LAX linalg layer) in
        \url{https://github.com/google/jax/blob/master/jax/lax/lax.py};
        a lot of \co{defjvp*}, one for each primitive, i.e. only forward
    \item numpy API def
        \url{https://github.com/google/jax/blob/master/jax/numpy/lax_numpy.py}
    \item they don't define explicit VJPs (for reverse mode), instead they use
        the forward trace, which has to be done anyway, and then run that
        backwards, transposing the JVP operations to get a reverse operation
\end{itemize}

\section{Tracing and compilation}

\subsection{\jax}

\jax uses an intermediate representation
dubbed \co{jaxpr} which is the result of tracing a function. Also every
other function, e.g. \co{grad(f)} is represented in \co{jaxpr}.

\begin{minted}{python}
    >>> import jax; from jax import numpy as jnp
    >>> f=lambda x: jnp.sum(jnp.power(jnp.sin(x),2.0))
    >>> jax.make_jaxpr(f)(1.0)
    { lambda  ; a.
      let b = sin a
          c = pow b 2.0
          d = reduce_sum[ axes=() ] c
      in (d,) }

    >>> jax.make_jaxpr(jax.grad(f))(1.0)
    { lambda  ; a.
      let b = sin a
          c = pow b 1.0
          d = mul 2.0 c
          e = mul 1.0 d
          f = cos a
          g = mul e f
      in (g,) }
\end{minted}

It uses \tf's XLA compiler, which uses LLVM, to compile for CPU and GPU targets.

\subsection{\pytorch}

The result of \pytorch's tracing is some representation of the comp graph (ATM
IDK more details about that). \pytorch's jit creates something called
TorchScript, which is a serialized version of the graph. That can be exported
to disk, loaded into C++ using the libtorch C++ API and executed as a C++
program.


\bibliographystyle{ieeetr}
\bibliography{lit}

Also:

\begin{itemize}
    \item Talk by Matthew Johnson (former HIPS, now Google, \jax) about
        \url{https://github.com/HIPS/autograd}:
        \url{http://videolectures.net/deeplearning2017_johnson_automatic_differentiation}
\end{itemize}


\end{document}
