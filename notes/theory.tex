\documentclass[paper=a4,11pt,headsepline]{scrartcl}

\usepackage[margin=2cm]{geometry}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{minted}
\usepackage{booktabs}
\usepackage{url}
\usepackage[colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black,hyperfootnotes=true]{hyperref}

\newcommand{\ve}[1]{\ensuremath{\bm{\mathit{#1}}}}
\newcommand{\ma}[1]{\ensuremath{\bm{\mathbf{#1}}}}
\newcommand{\dd}{\text{d}}
\newcommand{\ra}{\ensuremath{\rightarrow}}
\newcommand{\la}{\ensuremath{\leftarrow}}
\newcommand{\pd}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\td}[2]{\dfrac{\dd #1}{\dd #2}}
\newcommand{\pdi}[2]{\partial #1/\partial #2}
\newcommand{\tdi}[2]{\dd #1/\dd #2}
\newcommand{\red}[1]{{\color{red}{#1}}}
\newcommand{\green}[1]{{\color{green}{#1}}}
\newcommand{\blue}[1]{{\color{blue}{#1}}}

\begin{document}

\section*{Methods for diff}

\begin{itemize}
    \item work out derivs by hand and code them
    \item call symbolic engines (Mathematica, Maple, Maxima, sympy) in your code
    \begin{itemize}
        \item special sub-type: manually construct computation graph by
            framework mini-language (TensorFlow, Theano) (e.g. `tf.while`,
            `tf.cond`), the framework then does symbolic derivs and thus
            creates a new computation graph for derivs which is used in SGD
    \end{itemize}
\item numerical derivs by FD (use at least central diffs, `numdifftools`), good
    for checking AD gradients, scales as $\mathcal O(n)$ for $\nabla f(\ve x),
    \ve x\in\mathbb R^n$, slow when $n\sim 10^6$
\item AD
    \begin{itemize}
        \item directly provides numerical values for gradients at some point
            $\ve x$ (e.g. $\nabla f|_{\ve x}$) with machine precision, *not*
            symbolic expressions of derivatives
        \item uses operator overloading (or other methods) to carry out derivs
        \item can be applied to "any" code using arbitrary control flows
        \item in reverse mode uses code tracing ("taping") by one forward eval
            to construct computation graph
        \item no "code swell" (complex expressions that grow which each
            application of $\partial/\partial x_i$ as in symbolic engines)
            which need an additional simplification step (Theano does this)
    \end{itemize}
\end{itemize}
%
\section*{Some basics}
First consider a scalar function of scalar inputs $f:\mathbb R\ra\mathbb R$
\begin{equation*}
    f(x) = c(b(a(x)))
\end{equation*}
which gives the straight graph $x\ra a\ra b\ra c = f$. Forward = $x \ra c$ (inputs
to outputs). Reverse = $x \la c$ (outputs to inputs).
The derivative using the chain rule
\begin{align*}
    \td{f}{x}
        &= \td{c}{b}\,\td{b}{a}\,\td{a}{x}\\
        &= \td{c}{b}\,\left(\td{b}{a}\,\td{a}{x}\right)\quad\text{forward}\\
        &= \left(\td{c}{b}\,\td{b}{a}\right)\,\td{a}{x}\quad\text{reverse}\\
\end{align*}

Now let's look at a multivariate function $f(\ve x): \mathbb R^n \ra \mathbb R$.
\begin{equation*}
    f(\ve x) = c(b(a(\ve x)))
\end{equation*}
The gradient using the chain rule is
\begin{equation*}
    \pd{f}{\ve x} = \pd{c}{b}\,\pd{b}{a}\,\pd{a}{\ve x}
\end{equation*}
We can now pick out the $i$-th derivative $\pdi{f}{x_i}$ by multiplication with
a vector
$\ve e_i = [0,0,\ldots,1,\ldots, 0]$ which we can write as $\pdi{\ve x}{x_i}$.
\begin{align*}
    \pd{f}{\ve x}\,\pd{\ve x}{x_i} = \pd{f}{x_i}
        &= \pd{c}{b}\,\pd{b}{a}\,\pd{a}{\ve x}\pd{\ve x}{x_i}\\
        &= \pd{c}{b}\,\pd{b}{a}\,\pd{a}{x_i}
\end{align*}
We will generalize this below.

\section*{Forward and reverse AD (in JAX)}
Now we consider a multivariate vector function $\ve f: \mathbb R^n \ra \mathbb R^m$.
The Jacobian is
\begin{equation*}
    \ve f(\ve x): \mathbb R^n \ra \mathbb R^m\quad \ma J =
        \begin{pmatrix}
            \pd{f_1}{x_1} & \cdots & \pd{f_1}{x_n}  \\
            \vdots        & \ddots & \vdots         \\
            \pd{f_m}{x_1} & \cdots & \pd{f_m}{x_n}
        \end{pmatrix}
    \in\mathbb R^{m\times n}
\end{equation*}
One edge case is
\begin{equation*}
    \ve f(x): \mathbb R^1 \ra \mathbb R^m\quad \ma J =
        \begin{pmatrix}
            \pd{f_1}{x_1}  \\
            \vdots         \\
            \pd{f_m}{x_1}
        \end{pmatrix}
    \in\mathbb R^{m\times 1}
\end{equation*}
where we need only the first column $\ma J[:,1]$. In the other edge case
\begin{equation*}
f(\ve x): \mathbb R^n \ra \mathbb R^1\quad \ma J =
    \begin{pmatrix}
        \pd{f_1}{x_1} & \cdots & \pd{f_1}{x_n}
    \end{pmatrix}
    \in\mathbb R^{1\times n}
    \equiv \nabla f\equiv \pd{f}{\ve x}
\end{equation*}
we need the first row $\ma J[1,:]$ only.

\subsection{Forward}

Again we have
\begin{equation*}
    \ve f(\ve x) = \ve c(\ve b(\ve a(\ve x)))
\end{equation*}
and can use the chain rule
\begin{equation*}
    \ma J=\pd{\ve f}{\ve x} = \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}
\end{equation*}
which would give a series of matrix multiplications of individual Jacobians $\pdi{\ve
a}{\ve x}$, $\pdi{\ve b}{\ve a}$, $\pdi{\ve c}{\ve b}$. Of course we don't want
to build up Jacobians. Also we haven't done any AD yet, have we?
The trick is to evaluate the expression by using ($i$) pre-defined derivatives of
\emph{primitive} functions and ($ii$) the $\ve e_i$ trick from above.

We can extract the $j$-th column $\ma J[:,j]$ by multiplying from the right
with $\partial\ve x/\partial x_j = \ve e_j = [0,0,\cdots,1,\cdots, 0]$ to
select the $x_j$ w.r.t. to which we want to calculate the derivative. We apply
\blue{Jacobian vector products (JVPs)} as we go.
\begin{align*}
    \pd{\ve f}{\ve x}\,\red{\pd{\ve x}{x_j}}
        &= \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\,\red{\pd{\ve x}{x_j}}\\
        &= \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\,\red{\ve e_j}\\
        &= \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\blue{\pd{\ve a}{\ve x}\,\ve e_j} \\
        &= \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\blue{\ve v_j} \\
        &= \pd{\ve c}{\ve b}\,\ve v_j' \\
        &= \ma J[:,j]
\end{align*}
Now what is a JVP? We see that in order to evaluate the chain rule,
each \emph{primitive} function ($\ve a(\cdot)$, $\ve b(\cdot)$, $\ve c(\cdot)$)
formally needs to know ($i$) how to evaluate it's own derivative (Jacobian, e.g.
$\pdi{\ve b}{\ve a}$) w.r.t. it's inputs and ($ii$) how to multiply that with a given vector.
By doing ($ii$), we can always initialize the process by $\ve e_j$ and then
multiply forward. Cool! Now how do we do $(i)$? There is a function \texttt{jax.jvp}
\begin{equation*}
    \texttt{vjp}: (\ve x, \ve v) \ra \left(\ve f(\ve x), \ma J(\ve x)\,\ve v\right)
\end{equation*}
which maps a pair of so-called \emph{primal} \ve x and \emph{tangent} \ve v to the value
of \ve f and the JVP. The primal \ve x is the point at which \ve f and \ma J are
to be evaluated, the tangent \ve v is the vector used in the JVP. This is very
roughly how JAX works inside. Each primitive (numpy) function has an associated
function defined by using \texttt{jax.devjvp} which gets called when evaluating
the chain rule. \textbf{The most important trick is that the intermediate Jacobians are
never explicitly build}. The JVP function only has to return a value
which is the \emph{result} of $\ma J(\ve x)\,\ve v$. Especially, for numpy's
vectorized functions, all Jacobians are diagonal (!), which makes coding up what
the JVP function has to do simple and efficient.

When we repeat the process and apply all possible $\ve e_j$, we can build up the
full $\ma J$ if we need to, one column at a time. For the edge case $\ve f(x): \mathbb R^1
\ra \mathbb R^m$ (first column) we are finished in one forward pass. In general
forward mode is efficient for "tall" Jacobians where $m\gg n$.

\subsection{Reverse}

First, we do a forward pass to trace the execution done in $\ve f$ to build up the graph
$\ve x\ra \ve a\ra \ve b\ra \ve c = \ve f$.
Then we can extract $i$-th row $\ma J[i,:]$ by multiplying from the left with
$\ve e_i$. This time, we apply \blue{vector Jacobian products (VJPs)} as we go.
\begin{align*}
    \red{\left(\pd{\ve x}{x_i}\right)^\top}\,\pd{\ve f}{\ve x}
        &= \red{\left(\pd{\ve x}{x_i}\right)^\top}\,\pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\\
        &= \red{\ve e_i^\top}\,\pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\\
        &= \blue{\ve e_i^\top\,\pd{\ve c}{\ve b}}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\\
        &= \ldots\\
        &= \ma J[i,:]
\end{align*}
Again, we can apply all possible $\ve e_i$ to build up full $\ma J$, this time one row at
a time. For the edge case $f(\ve x): \mathbb R^n \ra \mathbb R^1$ (first row)
we are done in one backward pass.

Note that
\begin{equation*}
    \ve e_i^\top\,\ma J = \ma J^\top\,\ve e_i
\end{equation*}
i.e. use JVPs on $\ma J^\top$. This is how JAX does it, as far as I can tell.
In contrast to autograd, in JAX
they never define VJPs but instead only JVPs and use the transpose trick
to evaluate the reverse mode.

\subsection{Reverse in ML a.k.a. backprop}

Typical in ML: a multivariate function $f(\ve x): \mathbb R^n\ra \mathbb R$
\begin{equation*}
    f(\ve x) = c(\ve b(\ve a(\ve x)))
\end{equation*}
for instance
\begin{minted}{python}
    f = lambda x: c(b(a(x)))
    f = lambda x: sum(power(sin(x),2))
\end{minted}
that is composed of a number of vector-valued functions $\ve a(\cdot), \ve
b(\cdot)$ and a final contraction $c(\cdot): \mathbb R^n\ra \mathbb R$ and where $n$ is "large" (as in $10^5$).
For example, $f$ is the loss function in NN training and $\ve x$ are the
network's parameters. This is exactly the second edge case from above. When
using reverse mode, we get the gradient $\pdi{f}{\ve x} = \ma J[1,:]$
in a single reverse pass (i.e. backprop).

\section*{Comparison of diff implementations}

% XXX vmap(grad) is not really grad!
\begin{table}[h]
    \begin{tabular}{lllll}
        \toprule
        what                  & scipy                        & numdifftools     & jax                      & autograd               \\
        \midrule
        $\dd f(x)/\dd x$      & \verb|M.gradient|            & \verb|Derivative|&   \verb|grad|            & \verb|grad|            \\
        $\nabla f(\ve x)$     & \verb|O.approx_fprime|       & \verb|Gradient|  &   ???\verb|vmap(grad(.))|   & ????\verb|elementwise_grad|\\
        $\dd\ve f/\dd\ve x$   & \verb|N.approx_derivative|   & \verb|Jacobian|  &   \verb|jacobian|        & \verb|jacobian|        \\
        \bottomrule
    \end{tabular}
    \caption{scipy: \texttt{M=scipy.misc}, \texttt{O=scipy.optimize}, \texttt{N=optimize.\char`_numdiff}}
\end{table}

\section*{Syntax}

JAX (and mostly also autograd)

% XXX not sure about argnums and df/dx_i, there is a diff between f(x) with x
% \in R^n and explicit f(x_1, x_2)
\begin{table}[h]
    \begin{tabular}{ll}
        \toprule
        math                                    &   code \\
        \midrule
        $\td{f}{x}$                             & \texttt{grad(f)}            \\
        ???$\pd{f(\ve x)}{x_i}$                    & \texttt{grad(f, argnums=i)} \\
        ??? fake $\nabla f$ (e.g. \texttt{f=np.sin})   & \texttt{vmap(grad(f))} \\
        $\ma J$                                 & \texttt{jacobian(f)}, \texttt{jacfwd(f)}, \texttt{jacrev(f)} \\
        \bottomrule
    \end{tabular}
    \caption{Syntax}
\end{table}

\section*{JVPs (forward) and VJPs (reverse) in autograd an JAX}

Autograd

\begin{itemize}
    \item They define a VJP for "each" numpy function in
        \url{https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_vjps.py}
        using \texttt{defvjp}, also some JVPs in
        \url{https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_jvps.py}
        but autograd is mostly only reverse mode
    \item numpy API definition
        \url{https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_wrapper.py}
\end{itemize}

JAX

\begin{itemize}
    \item numpy primitives defined (re-implemented using the LAX linalg layer) in
        \url{https://github.com/google/jax/blob/master/jax/lax/lax.py};
        a lot of \texttt{defjvp*}, one for each primitive, i.e. only forward
    \item numpy API def
        \url{https://github.com/google/jax/blob/master/jax/numpy/lax_numpy.py}
    \item they don't define explicit VJPs (for reverse mode), instead they use
        the forward trace, which has to be done anyway, and then run that
        backwards, transposing the JVP operations to get a reverse operation
\end{itemize}

Refs:

\begin{itemize}
    \item \url{https://insidehpc.com/2017/12/deep-learning-automatic-differentiation-theano-pytorch}
    \item \url{https://arxiv.org/abs/1502.05767}
    \item \url{http://www.autodiff.org}
    \item Talk by Matthew Johnson (former HIPS, now Google, JAX) about
        \url{https://github.com/HIPS/autograd}:
        \url{http://videolectures.net/deeplearning2017_johnson_automatic_differentiation}
\end{itemize}

\end{document}
