% vim:tw=150

\documentclass[fleqn,10pt]{beamer}

%----------------------------------------------------------------------------------%
% packages, lit
%----------------------------------------------------------------------------------%

\usepackage{bm}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage[
    backend=biber,
    maxbibnames=2,
    giveninits=true,
    url=true,
    isbn=false,
    sorting=none,
    date=year,
    ]{biblatex}
\addbibresource{lit.bib}


%----------------------------------------------------------------------------------%
% minted
%----------------------------------------------------------------------------------%


\tcbuselibrary{minted}

\tcbset{%
    % must use \tcbuselibrary{minted}
    %%listing engine=minted,
    minted options={fontsize=\small},
    minted style=gruvbox-light,
    listing only,
    boxsep=0pt,
    leftrule=0pt,
    rightrule=0pt,
    toprule=0pt,
    bottomrule=0pt,
    right=2pt,
    top=2pt,
    bottom=2pt,
}

\newtcbinputlisting[]{\mintedfromfile}[2]{%
    minted language=#1,
    listing file={#2},
    left=20pt,
}

\newtcblisting{mintedcode}[1]{%
    minted language=#1,
    left=0pt,
}

%----------------------------------------------------------------------------------%
% beamer style and layout
%----------------------------------------------------------------------------------%

\defbeamertemplate{footline}{megamega}{%
    \usebeamercolor[fg]{page number in head/foot}%
    \usebeamerfont{page number in head/foot}%
%%    \hspace{1em}Important Meeting XY \hspace{1em} My name \hspace{1em} My Department
    \hfill%
    \insertframenumber\,/\,\inserttotalframenumber\kern1em\vskip2pt%
}
\setbeamertemplate{footline}[megamega]{}

\setbeamerfont{footline}{size=\fontsize{6}{6}\selectfont}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{caption}{\insertcaption}

% For xelatex: force nice standard CMR font
\usepackage{lmodern}

% No standard beamer font, thank you very much
\usefonttheme{serif}


%----------------------------------------------------------------------------------%
% newcommands
%----------------------------------------------------------------------------------%

\include{nc}


%----------------------------------------------------------------------------------%
% title & stuff
%----------------------------------------------------------------------------------%

\title{Autodiff 101}
\subtitle{Introduction to Automatic Differentiation with examples from \jax and \pytorch}

\author{Steve Schmerler}
\institute{HZDR/Helmholtz AI}

\date{}

%----------------------------------------------------------------------------------%

\begin{document}


\newcommand{\edgecol}{
    \begin{equation*}
        \ve f: \mathbb R \ra \mathbb R^m\quad \ma J
        = \pd{\ve f}{x} =
        \begin{bmatrix}
            \pd{f_1}{x_1}  \\
            \vdots         \\
            \pd{f_m}{x_1}
        \end{bmatrix}
        = \ma J[:,1]\in\mathbb R^{m\times 1}
    \end{equation*}
}

\newcommand{\edgerow}{
    \begin{equation*}
        f: \mathbb R^n \ra \mathbb R\quad \ma J
        = \pd{f}{\ve x} =
        \begin{bmatrix}
            \pd{f_1}{x_1} & \cdots & \pd{f_1}{x_n}
        \end{bmatrix} \equiv \nabla f
        = \ma J[1,:] \in\mathbb R^{1\times n}
    \end{equation*}
}


\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Motivation}
    \begin{block}{Why derivatives?}
        In DL: $f(\ve x) = \mathcal L(\ve\theta),\quad f:\mathbb R^n\ra\mathbb R,\quad \min_{\ve x}f(\ve x)$ using SGD $\ra\nabla_{\ve x} f$
    \end{block}
    \uncover<2->{
        \begin{block}{How to get $\nabla f$}
            \begin{itemize}
                \item work out derivatives by hand (or really \soft{Mathematica}, \sympy, ...) and code them
                \item call symbolic engine (\soft{sympy}, ...)
                \item use finite diff approximation $\pdi{f}{x_j}\approx[f(\ve x +\ve e_j\,h) - f(\ve x-\ve e_j h)]/2\,h$
                \item AD: diff through mathematical operations in (arbitrary) source code using
                \begin{itemize}
                    \item chain rule
                    \item pre-defined derivatives of elementary ("\emph{primitive}") functions
                \end{itemize}
                \item in NNs: "backprop" = reverse mode AD
            \end{itemize}
        \end{block}
    }
    \uncover<3->{
        \begin{columns}
            \column{0.5\textwidth}
                \centering
                \includegraphics[width=0.8\textwidth]{pics/pytorch-logo-dark.png}
            \column{0.5\textwidth}
                \centering
                \includegraphics{pics/jax_logo_250px.png}\\
                \url{github.com/google/jax}
        \end{columns}
    }
\end{frame}


\begin{frame}[fragile]
    \frametitle{Chain rule}
    Nested functions:
    \begin{equation*}
        f(x) = c(b(a(x)))\quad f = c \circ b \circ a
    \end{equation*}
    Computation graph:
    \begin{align*}
        x\ra a\ra b\ra c &= f\quad\text{"forward"}\\
        x\la a\la b\la c &= f\quad\text{"backward/reverse"}
    \end{align*}
    Example:
    \begin{equation*}
        f(x) = \ln(\sin^2(x))
    \end{equation*}
    Code:
    \begin{mintedcode}{python}
    f = lambda x: np.log(np.power(np.sin(x), 2))
    \end{mintedcode}
    Derivative:
    \begin{equation*}
        \td{f}{x} = \red{\td{c}{b}}\,\green{\td{b}{a}}\,\blue{\td{a}{x}}
            = \red{\frac{1}{\sin^2(x)}}\,\green{2\,\sin(x)}\,\blue{\cos(x)}
            = 2\,\frac{\cos(x)}{\sin(x)} = 2\,\cot(x)
    \end{equation*}
    Code:
    \begin{mintedcode}{python}
    fprime = lambda x: 2 * np.cos(x) / np.sin(x)
    \end{mintedcode}
\end{frame}


\begin{frame}
    \frametitle{AD teaser: arbitrary code}
    \only<1>{
        \mintedfromfile{python}{code/jax_ad_teaser_func.py}
        \vfill
    }
    \only<2->{
        \uncover<2->{
            \mintedfromfile{python}{code/jax_ad_teaser_func_w_imports.py}
        }
    }
    \vspace{-0.3cm}
    \uncover<3->{
        \mintedfromfile{python}{code/jax_ad_teaser_grad_usage.py}
    }
\end{frame}


\begin{frame}[fragile]
    \frametitle{Jacobian}
    \begin{equation*}
        \ve f: \mathbb R^n \ra \mathbb R^m, \ve x\mapsto \ve f(\ve x) = \left[f_1(\ve x) \cdots f_m(\ve x)\right]^\top
    \end{equation*}
    \begin{equation*}
        \ve f(\ve x) = \ve c(\ve b(\ve a(\ve x)))
    \end{equation*}
    For $n=m$:
    \begin{mintedcode}{python}
    f = lambda x: np.log(np.power(np.sin(x), 2))
    \end{mintedcode}
    \vfill
    \uncover<2->{
        Jacobian:
        \begin{equation*}
            \ve f: \mathbb R^n \ra \mathbb R^m,\quad \ma J
            = \pd{\ve f}{\ve x} =
            \begin{bmatrix}
                \pd{f_1}{x_1} & \cdots & \pd{f_1}{x_n}  \\
                \vdots        & \ddots & \vdots         \\
                \pd{f_m}{x_1} & \cdots & \pd{f_m}{x_n}
            \end{bmatrix}
            \in\mathbb R^{m\times n}
        \end{equation*}
        Note: $\pd{\ve x}{\ve x} = (\delta_{ij})$ is the identity matrix
    }
\end{frame}

\begin{frame}
    \frametitle{Jacobian edge cases}
    $n=1$
    \edgecol
    \uncover<2>{
        $m=1$
        \edgerow
    }
\end{frame}


\begin{frame}
    \frametitle{Forward mode AD}
    \begin{equation*}
        \ve f: \mathbb R^n \ra \mathbb R^m,\quad\ve f(\ve x) = \ve c(\ve b(\ve a(\ve x)))
    \end{equation*}
    \begin{equation*}
        \ma J=\pd{\ve f}{\ve x} = \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}
    \end{equation*}
    \vfill
    \uncover<2->{
        Right-multiply with identity matrix $\blue{\pdi{\ve x}{\ve x}}$
        \begin{equation*}
            \pd{\ve f}{\ve x}\,\blue{\pd{\ve x}{\ve x}} =
            \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\,
                \blue{\pd{\ve x}{\ve x}}
        \end{equation*}
    }
    \vfill
    \uncover<3->{
        Right-multiply with one column $\pdi{\ve x}{x_j} = \ve e_j = [0,0,\cdots,1,\cdots, 0]^\top$ selects $j$th column $\ma J[:,j]$
    }
\end{frame}

\begin{frame}
    \frametitle{Forward mode AD: Jacobian-vector products}
    \begin{align*}
        \pd{\ve f}{\ve x}\,\blue{\pd{\ve x}{x_j}} = \pd{\ve f}{\ve x}\,\blue{\ve e_j}
            \uncover<2->{
                &= \pd{\ve c}{\ve b}\,\pd{\ve b}{\ve a}\,\red{\pd{\ve a}{\ve x}}\,\blue{\ve e_j} \\
            }
            \uncover<3->{
                &= \pd{\ve c}{\ve b}\,\red{\pd{\ve b}{\ve a}}\,\blue{\ve v_j} \\
            }
            \uncover<4->{
                &= \red{\pd{\ve c}{\ve b}}\,\blue{\ve v_j'} \\
            }
            \uncover<5->{
                &= \ma J[:,j]
            }
    \end{align*}
    \uncover<6->{
        Augment each intermediate $\ve z(\cdot) = \ve a(\cdot), \ve b(\cdot), \ve c(\cdot)$ with
        \begin{equation*}
            \text{JVP: }\red{\pd{\ve z}{\ve\tau}}\,\blue{\ve v}
        \end{equation*}
    }
    \uncover<7->{
        {\small
        \begin{itemize}
            \item $\ve z(\cdot)$ and JVP evaluated together \emph{forward} along graph $x\ra a\ra b\ra c$
            \item build graph on the fly
            \item define JVP for each primitive \numpy func $\ve z(\cdot)$
            \item $\red{\pdi{\ve z}{\ve\tau}}$ is \emph{never} build, diagonal for most \numpy funcs, JVPs are simple expressions (e.g. element-wise ops on
                $\ve v$)
        \end{itemize}}
    }
\end{frame}

\begin{frame}
    \frametitle{Custom JVPs in \jax}
    \only<1>{
        \mintedfromfile{python}{code/jax_defjvp_mysin_no_decorator.py}
    }
    \only<2->{
        \uncover<2->{
            \mintedfromfile{python}{code/jax_defjvp_mysin.py}
        }
    }
    \vspace{-0.3cm}
    \uncover<3->{%
        \mintedfromfile{python}{code/jax_defjvp_with_jac.py}
    }
    \vspace{-0.3cm}
    \uncover<4->{%
        \mintedfromfile{python}{code/jax_defjvp.py}
    }
\end{frame}

\begin{frame}
    \frametitle{Default JVPs for \numpy primitives in \jax}
    \mintedfromfile{python}{code/jax_lax_sin.py}
\end{frame}

\begin{frame}
    \frametitle{Properties of forward mode AD}
    Apply all $\ve e_j$ \ra build $\ma J$ \emph{one column at a time}.
    Efficient for "tall" $\ma J$ with $m\gg n$, best: single forward pass for $\ve f(x): \mathbb R \ra \mathbb R^m$ ($n=1$, first column)
    \edgecol
    \uncover<2->{
        Inefficient for "wide" $\ma J$ with $m\ll n$, worst: $n$ forward passes for $f(\ve x): \mathbb R^n \ra \mathbb R$ ($m=1$, first row)
        \edgerow
        Same as FD: $n$ times do $\pdi{f}{x_j}\approx[f(\ve x +\ve e_j\,h) - f(\ve x-\ve e_j h)]/2\,h$.
    }
\end{frame}

\begin{frame}
    \frametitle{Reverse mode AD: vector-Jacobian products}
    Multiplying from the left with one row $(\pdi{\ve f}{f_i})^\top = \ve e_i^\top$, extract $i$th row $\ma J[i,:]$
    \begin{align*}
        \blue{\left(\pd{\ve f}{f_i}\right)^\top}\,\pd{\ve f}{\ve x} = \blue{\ve e_i^\top}\,\pd{\ve f}{\ve x}
            &= \blue{\ve e_i^\top}\,\red{\pd{\ve c}{\ve b}}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\\
            &= \blue{\ve v_i^\top}\,\red{\pd{\ve b}{\ve a}}\,\pd{\ve a}{\ve x}\\
            &= \blue{\ve v_i'^\top}\,\red{\pd{\ve a}{\ve x}}\\
            &= \ma J[i,:]
    \end{align*}
    \uncover<2->{
        Augment each intermediate $\ve z(\cdot) = \ve a(\cdot), \ve b(\cdot), \ve c(\cdot)$ with
        \begin{equation*}
            \text{VJP: }\blue{\ve v^\top}\,\red{\pd{\ve z}{\ve\tau}}
        \end{equation*}
    }
    \uncover<3->{
        {\small
        \begin{itemize}
            \item 2 phases
            \begin{itemize}
                \item (1) forward pass to trace graph $x\ra a\ra b\ra c$ ("taping")
                \item (2) backward pass: apply VJPs
            \end{itemize}
            \item define VJPs for each primitive \numpy func $\ve z(\cdot)$
        \end{itemize}}
%%        Note that
%%        \begin{equation*}
%%            \ve e_i^\top\,\ma J = \ma J^\top\,\ve e_i
%%        \end{equation*}
    }
\end{frame}

\begin{frame}
    \frametitle{Reverse mode VJPs in \pytorch}
    \url{https://github.com/pytorch/pytorch/blob/main/tools/autograd/derivatives.yaml}
    \mintedfromfile{yaml}{code/pytorch_vjp.yaml}
    Scripts to generate C++ code (\co{libtorch.so}) and Python bindings.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Properties of reverse mode AD}
    Apply all $\ve e_i$ \ra build $\ma J$ \emph{one row at a time}.
    Efficient for "wide" $\ma J$ with $m\ll n$, best: one single reverse pass for $f(\ve x): \mathbb R^n \ra \mathbb R$ ($m=1$, first row)
    \edgerow
    \uncover<2->{
        DL: loss function $f(\ve x): \mathbb R^n\ra \mathbb R$
        \begin{equation*}
            f(\ve x) = c(\ve b(\ve a(\ve x)))
        \end{equation*}
        \mintedfromfile{python}{code/scalar_field.py}
    }
\end{frame}

%%\begin{frame}[fragile]
\begin{frame}
    \frametitle{\pytorch: Reverse mode example}
    \begin{equation*}
        f: \mathbb R^n\ra \mathbb R, \quad f(\ve x) = c(\ve b(\ve a(\ve x)))
    \end{equation*}
    \mintedfromfile{python}{code/pytorch_fwd_rev_1.py}
    \uncover<2->{
        Forward pass, tracing, observe \texttt{grad\textunderscore fn}
        \mintedfromfile{python}{code/pytorch_fwd_rev_2.py}
    }
    \uncover<3->{
        Backward pass
        \mintedfromfile{python}{code/pytorch_fwd_rev_3.py}
    }
\end{frame}

\begin{frame}
    \frametitle{\pytorch: Reverse pass details}
    \begin{equation*}
        f: \mathbb R^n\ra \mathbb R, \quad f(\ve x) = c(\ve b(\ve a(\ve x)))
    \end{equation*}
    \begin{align*}
        \blue{\pd{f}{f}}\,\pd{f}{\ve x} = \blue{1}\,\pd{f}{\ve x}
            &= \blue{1}\,            \red{\pd{c}{\ve b}}\,\pd{\ve b}{\ve a}\,\pd{\ve a}{\ve x}\\
            &= \blue{\ve v_i^\top}\, \red{\pd{\ve b}{\ve a}}\,\pd{\ve a}{\ve x}\\
            &= \blue{\ve v_i'^\top}\,\red{\pd{\ve a}{\ve x}}
    \end{align*}
    \uncover<2->{
        Default: initialize backward pass with $\pdi{f}{f}=1$
        \mintedfromfile{python}{code/pytorch_rev_detail_1.py}
    }
    \uncover<3->{
        VJP: extract one row of Jacobian $\pdi{\ve b}{\ve a}$
        \mintedfromfile{python}{code/pytorch_rev_detail_2.py}
    }
\end{frame}

\begin{frame}[fragile]
    \frametitle{Higher order fun}
    \mintedfromfile{python}{code/higher_order_1.py}
    \vspace{-0.3cm}
    \uncover<2->{
        \mintedfromfile{python}{code/higher_order_2.py}
    }
    \uncover<3->{
        $\ma J:=\partial f; \ma H = \partial^2 f = \partial(\partial f) = \partial(\nabla f)$ for $f:\mathbb R^n\ra\mathbb R$
    }
    \uncover<4->{
        \mintedfromfile{python}{code/higher_order_hessian.py}
    }
\end{frame}


\begin{frame}[fragile]
    \frametitle{No free lunch}
    \begin{itemize}
        \item restricted to \numpy/\pytorch-based code or need to wrap target package
        \item in-place ops (\co{a[i] *= 3}) can break AD, \pytorch: Tensor versioning system
        \begin{mintedcode}{python}
    RuntimeError: one of the variables needed for gradient
    computation has been modified by an inplace operation:
    [torch.FloatTensor []], which is output 0 of
    SelectBackward, is at version 55; expected version 51
    instead. Hint: enable anomaly detection to find the
    operation that failed to compute its gradient, with
    torch.autograd.set_detect_anomaly(True).
        \end{mintedcode}
        \item numerical accuracy a.k.a. when do you want \verb|custom_jvp|
        \begin{itemize}
            \item AD derivatives can generate numerically unstable code
            \item example: ADing fast but inaccurate approximations of functions (e.g. \co{exp(x)}): "approximate the derivative, not differentiate
                the approximation"
        \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}
    \nocite{*}
    \printbibliography
\end{frame}

\begin{frame}
    \frametitle{\texttt{jaxpr}}
    \mintedfromfile{python}{code/jaxpr.py}
\end{frame}

\end{document}
